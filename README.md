# Projet RATP üöá


### Avant-Propos
Ce projet a √©t√© r√©alis√© dans le cadre du cours de "Syst√®mes r√©partis" dispens√© par Antoine Monino. Le but √©tait d'effectuer un projet de Data Science, de l'extraction des donn√©es √† la mod√©lisation. Ce projet s'est av√©r√© particuli√®rement formateur car nous avons pu appr√©hender toute une partie "traitement de donn√©es massives" sur un environnement Big Data de type cluster Hadoop.


### Probl√©matique

Le trafic ferroviaire de l'Ile de France est sujet √† de nombreuses critiques de la part des usagers. Les incidents d√©clar√©s chaque jour, les saturations de certaines lignes aux heures de pointe, ainsi que les diff√©rentes in√©galit√©s d'acc√®s √† la capitale sont autant de probl√®mes qui n√©cessitent des am√©liorations.

Dans ce contexte, nous avons choisi de nous pencher sur la RATP, car nous sommes quotidiennement confront√©s √† ces probl√©matiques. Notre projet a pour but d'√©tudier l'√©volution du trafic annuel des diff√©rentes stations de RER et de m√©tro parisiens.

Nous avons initialement envisag√© d'√©tudier les incidents sur les diff√©rentes lignes, car les donn√©es sur ces incidents sont riches en informations. Nous aurions pu comprendre les causes profondes des incidents en faisant du text mining, cat√©goriser les incidents et pr√©voir les temps de r√©solution. Cependant, nous avons d√ª √©carter cette piste car l'extraction des donn√©es s'av√©rait complexe en utilisant un notebook.

Notre probl√©matique reste tout de m√™me int√©ressante car nous tentons d'identifier les facteurs qui expliquent l'√©volution du trafic annuel, tels que la qualit√© du service, la g√©ographie, l'information des voyageurs ou le fait d'√™tre desservi par une ligne en particulier. Nous esp√©rons que ce projet sera utile pour les usagers de la RATP et pour tous ceux qui s'int√©ressent √† la mobilit√© en Ile de France.


### Les donn√©es
Afin de r√©aliser ce projet nous avons extrait nos donn√©es de  [PRIM](https://prim.iledefrance-mobilites.fr/fr) et  [RATP](https://data.ratp.fr/explore/).
Comme √©nonc√© dans la guideline, √† diff√©rentes √©tapes du projet, il y a diff√©rents jeux de donn√©es:

##### Description de la base de donn√©es SQL 
La base SQL de notre projet 'mosef_projet_ratp'  comporte initialement 7 tables. Pour chaque table, vous avez la possibilit√© de voir la description du jeu de donn√©es en cliquant sur le nom de la table:

- [arrets](https://prim.iledefrance-mobilites.fr/fr/donnees-statiques/arrets-lignes)
- [emplacement_gare](https://prim.iledefrance-mobilites.fr/fr/donnees-statiques/emplacement-des-gares-idf-data-generalisee)
- [indicateur_qualite](https://eu.ftp.opendatasoft.com/stif/QualiteService/presentation_des_indicateurs_qs_opendata.pdf)
- [lignes_actives_2021](https://eu.ftp.opendatasoft.com/stif/Documentation/R√©f√©rentiels.pdf)
- [projets](https://prim.iledefrance-mobilites.fr/fr/donnees-statiques/projets_lignes_idf)
- [trafic_2020](https://data.ratp.fr/explore/dataset/trafic-annuel-entrant-par-station-du-reseau-ferre-2020/information/)
- [trafic_2021](https://data.ratp.fr/explore/dataset/trafic-annuel-entrant-par-station-du-reseau-ferre-2021/information/)


##### Description du jeu de donn√©es pour l'analyse et mod√©lisation

Comme expliqu√© ensuite, nous avons r√©alis√© diff√©rentes jointures entre les diff√©rentes tables de la base SQL (les √©tapes sont expliqu√©s dans cette section). Ainsi, finalement, on obtient une table qui sera notre jeu de donn√©es pour la mod√©lisation. Les observations de ce jeu de donn√©es correspondent aux diff√©rentes stations de m√©tros, RER, tramway, transiliens (relatif √† la RATP ainsi qu'√† la SNCF pour le RER hors Paris et les transiliens). Pour chaque station, on dispose de diff√©rentes variables:
- stop_name : nom de la station (STRING) 
- stop_lon : longitude de la station (FLOAT)
- stop_lat : latitude de la station (FLOAT)
- OperatorName: Nom de l'op√©rateur c'est-√†-dire SNCF ou RATP (STRING) 
- ligne_name: Nom de la ligne principale comme M√©tro 1 ou RER A (STRING) 
- Contact_voyageurs: Score de qualit√© portant sur la capacit√© √† pouvoir contacter en cas de probl√®me (pour les voyageurs) 
- Information_voyageurs: Score de qualit√© portant sur la capacit√© √† pouvoir informer les voyageurs
- Propret√©:  Score de qualit√© portant sur la propret√© de la station
- S√ªret√©:  Score de qualit√© portant sur la s√ªret√© de la station
- ville: Ville dans laquelle se situe la station
- trafic_2020: trafic annuel en 2020 pour la station
- trafic_2021: trafic annuel en 2021 pour la station
- sum_correspondance: nombre de correspondance au sein de la station 




### Installation du projet :

- Connexion √† votre cluster Hadoop
- T√©l√©charger le repo du projet GitHub:
``` bash
#Typiquement
wget https://github.com/luciegaba/RATP-data-analysis-project/archive/refs/heads/main.zip -O /tmp/RATP/
unzip /tmp/RATP/main.zip -d .

```
- **Installation de la base de donn√©es**: connectez-vous √† votre cluster Hadoop, puis:
  - Initiez un fichier .env comportant:
  ```
  ssh_username=[votre_username]
  ssh_password=[votre_mot_de_passe]
  project_path=[path_projet_github: exemple = [/tmp/RATP/RATP-data-analysis-project-main/]
  ```
  - Lancez ce [script](https://github.com/luciegaba/RATP-data-analysis-project/blob/main/scripts/01_create_database/sql_loader/creating_hive_db.sh):
  ```
  cd project_path
  bash /scripts/01_create_database/sql_loader/creating_hive_db.sh
  ```
  
- Cr√©ation de la table utilis√©e pour l'analyse et la mod√©lisation:
``` shell
spark-submit project_path/01_create_database/create_table_for_analysis/create_table_for_analysis.py
```
- Processing & Mod√©lisation 
``` shell
spark-submit project_path/02_data_analysis/processing.py

```
  
### Guideline du projet

Le projet est constitu√© de diff√©rentes parties: 

- Constitution de la base de donn√©es SQL (via SQL & Shell)   *01_create_database*
- Cr√©ation de la table utilis√©e pour l'analyse et la mod√©lisation (via PySpark)   *01_create_database*
- Analyse descriptive de la base (via SparkSQL)    *02_data_analysis*
- Processing & Mod√©lisation (via Pyspark et MLLIB )   *02_data_analysis*


#### Constitution de la base de donn√©es 


Pour cette partie, notre ambition √©tait de cr√©er une base de donn√©es SQL disposant de plusieurs tables et ce, obtenues de diff√©rentes fa√ßons: t√©l√©chargement de donn√©es statiques mais √©galement extraction par appel API Real Time pour alimenter une table progressivement. Finalement, uniquement la premi√®re m√©thode a √©t√© utilis√© par faute de faisabilit√©. En effet, la seconde alternative n√©cessitait l'utilisation de Kafka, ainsi que de Python si nous voulions r√©aliser facilement l'extraction. 

Apr√®s avoir t√©l√©charg√© des donn√©es statiques sous forme de csv, on les d√©pose sur HDFS puis les chargeons dans les diff√©rentes tables de notre base SQL (pr√©alablement cr√©√©es)
Notre souhait √©tait vraiment de miser sur le c√¥t√© "Data Engineering" en cr√©ant une base donn√©es coh√©rente et utile √† diff√©rentes fins. 

Remarque: cette base dispose de donn√©es sur diff√©rents types de transports (bus, m√©tro, train), sur des lignes diff√©rentes, et des secteurs diff√©rents. Nous ciblons notre projet sur Paris et sa proche banlieue mais il y a diff√©rents √©lements qui peuvent √™tre √©galement √©tudi√©s.  




#### Cr√©ation de la table pour l'analyse et la mod√©lisation

Cette √©tape consistait √† cr√©er un jeu de donn√©es qui puisse √™tre exploitable pour un quelconque projet Data comme le notre. En effet, les tables disponibles dans notre base SQL sont brutes et ne peuvent constituer une source de donn√©es pertinente √† elles-seules. En ce sens, dans cette partie, nous avons effectu√© quelques modifications ainsi que des jointures pour avoir un niveau d'information satisfaisant. Toute cela a √©t√© r√©alis√© sur Spark. La table issue de cette √©tape est une table temporaire (une vue), qui est ensuite r√©cup√©r√©e dans l'√©tape suivante. 

#### Analyse descriptive de la base (via SparkSQL)
Cette √©tape consiste √† simplement constater diff√©rentes relations entre les variables dans le but d'effectuer une mod√©lisation ensuite. Cette analyse est centr√©e sur une variable d'inter√™t, le trafic annuel, avec laquelle on effectue des graphiques avec les autres variables de notre table.

#### Processing & Mod√©lisation (via Pyspark et MLLIB)

Le processing consiste au traitement de la target (cr√©ation de bins pour faire de la classification et non de la r√©gression), ainsi que des diff√©rentes variables explicatives. 

La mod√©lisation choisie est l'explication de l'√©volution du trafic annuel (sous forme de classification) √† savoir si l'√©volution entre 2020 et 2021 a √©t√© forte, faible, ou quasi inexistante OU si l'√©volution a √©t√© positive, n√©gative. 
Remarque: Ce mod√®le n'a pas pour but de pr√©dire l'√©volution du trafic, mais plut√¥t de comprendre les causes: il s'agit d'un mod√®le explicatif et non pr√©dictif. 




